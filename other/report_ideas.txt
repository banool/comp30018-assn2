Look at the results of instances where all features are not present (zero) in the feature vector. Did the classifier classify them correctly? If yes, was it just dumb luck? If no, why? Consider using their ID to find the real tweet and seeing which words that weren't used as features could indicate the correct class. Following this, discuss the trade off when using fewer features. Because it's so fast, look into using more features and seeing if the results (accuracy, precision, recall?) change.

Note the recall, why is the recall for SD so freakishly large compared to the other classes, as well as the F1-score being much higher. Why is the precision lower accordingly? Note that in 446, this effect is diminished, why?





This report considers different metrics and techniques for evaluating the performance of classifiers. Those mainly considered are precision and recall, including the relationship between them, and F-score, particularly variations upon it. FURTHER METRICS YES? Once metrics for evaluation are established, they are used to compare the two main classifiers in question, Naive Bayes and Support Vector Machine (SVM), in which the reasons for their performance are evaluated and their weaknesses are highlighted. As an extension, various methods to improve their performance are explored, namely boosting for Naive Bayes and kernel selection for SVM. ARE WE INTERESTED IN NUM OF FEATURES INCREASING OR NOT


  
    PROBLEMS WITH F-SCORE
    http://www.kdd.org/exploration_files/v12-1-p49-forman-sigkdd.pdf page 6

  EVALUATION OF CLASSIFIER PERFORMANCE USING F-SCORE

    (Because the F1 measure ignores true negatives and its magnitude is mostly determined by the number of true positives,


TITLE LIKE: APPLICATION OF EVALUATIVE METRICS

    NAIVE BAYES

    SUPPORT VECTOR MACHINE (SVM)

             precision    recall  f1-score   support

          B       0.61      0.09      0.16     12061
          H       0.44      0.15      0.23     12578
         SD       0.29      0.90      0.43     17929
         Se       0.69      0.09      0.16     14482
          W       0.54      0.19      0.29     11482

avg / total       0.50      0.33      0.27     68532

  USING MORE FEATURES ???

  FURTHER EVALUATIVE METRICS

MAYBE LOOK AT 35 FOR ALL AND THEN HAVE A SECTION LOOKING AT 446.
MAYBE FIND RESEARCH SUPPORTING THE CLAIM THAT PRECISION IS MORE IMPORTANT THAN RECALL FOR THIS AND MAKE AN F-SCORE AS SUCH. THEN YOU CAN JUST LOOK AT F-SCORE TO COMPARE.

TALK ABOUT BASELINES AND BENCHMARKS. TALK ABOUT BIAS AND VARIANCE.

THIS REPORT CONSIDERS HOW BEST TO EVALUATE THE PERFORMANCE OF A GEOLOCATION CLASSIFIER, LOOKING INITIALLY AT STANDARD METRICS BEFORE DEVELOPING METRICS TAILORED SPECIFICALLY FOR THIS TASK.

DO THE , average="micro" IN THE EVALUATION UNDER parameterizableReport

SHOW F1 SCORE FOR SOME TWO CLASSIFIERS THEN SHOW F0.5 SCORE AND SEE THE DIFFERENCE.

INDUCTIVE LEARNING HYPOTHESIS: any hypothesis found ot approximate the target function well over (a sufficiently large) training data set will also approximate the target function well over held out test examples.

talk about how fscore saves us looking at roc graphs?

why naive bayes and svm?



MAYBE LOOK AT BOTH MICRO AND MACRO see answer here for why: http://stats.stackexchange.com/questions/156923/should-i-make-decisions-based-on-micro-averaged-or-macro-averaged-evaluation-mea

ERROR RATE?